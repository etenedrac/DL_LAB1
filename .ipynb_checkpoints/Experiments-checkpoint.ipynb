{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version 2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from tensorflow import keras\n",
    "print( 'Using Keras version', keras.__version__)\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "x_valid = np.load('x_valid.npy')\n",
    "y_valid = np.load('y_valid.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "#Normalize data\n",
    "x_train = x_train.astype('float32')\n",
    "x_valid = x_valid.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / 255\n",
    "x_valid = x_valid / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_valid = to_categorical(y_valid, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "#MNIST resolution\n",
    "img_rows, img_cols, channels = 32, 32, 3\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], channels, img_rows, img_cols)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], channels, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], channels, img_rows, img_cols)\n",
    "    input_shape = (channels, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channels)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], img_rows, img_cols, channels)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channels)\n",
    "    input_shape = (img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "classes = sorted(os.walk('./train/').__next__()[1])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check sizes of dataset\n",
    "print( 'Number of train examples', x_train.shape[0])\n",
    "print( 'Size of train examples', x_train.shape[1:])\n",
    "\n",
    "#Adapt the data as an input of a fully-connected (flatten to 1D)\n",
    "x_train = x_train.reshape(90000, 3072)\n",
    "x_test = x_test.reshape(90000, 3072)\n",
    "\n",
    "#Normalize data\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#x_train = x_train / 255\n",
    "#x_test = x_test / 255\n",
    "\n",
    "#Adapt the labels to the one-hot vector syntax required by the softmax\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "#Define the NN architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "#Two hidden layers\n",
    "nn = Sequential()\n",
    "nn.add(Dense(64,activation='relu',input_shape=(3072,)))\n",
    "nn.add(Dense(32,activation='relu'))\n",
    "nn.add(Dense(32,activation='relu'))\n",
    "nn.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#Model visualization\n",
    "#We can plot the model by using the ```plot_model``` function. We need to install *pydot, graphviz and pydot-ng*.\n",
    "#from keras.util import plot_model\n",
    "#plot_model(nn, to_file='nn.png', show_shapes=true)\n",
    "\n",
    "#Compile the NN\n",
    "nn.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Starting tensorboard\n",
    "cbacks = []\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "cbacks.append(tensorboard)\n",
    "\n",
    "modfile = './model%d.h5' % int(time())\n",
    "mcheck = ModelCheckpoint(filepath=modfile, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "cbacks.append(mcheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start training\n",
    "history = nn.fit(x_train,y_train,batch_size=128,epochs=20, verbose=1, callbacks=cbacks)\n",
    "\n",
    "#Evaluate the model with test set\n",
    "score = nn.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test accuracy:', score[1])\n",
    "\n",
    "##Store Plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig('mnist_fnn_accuracy.pdf')\n",
    "plt.close()\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig('mnist_fnn_loss.pdf')\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "Y_pred = nn.predict(x_test)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print( 'Analysis of results' )\n",
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
    "\n",
    "# #Saving model and weights\n",
    "# from keras.models import model_from_json\n",
    "# nn_json = nn.to_json()\n",
    "# with open('nn.json', 'w') as json_file:\n",
    "#         json_file.write(nn_json)\n",
    "# weights_file = \"weights-MNIST_\"+str(score[1])+\".hdf5\"\n",
    "# nn.save_weights(weights_file,overwrite=True)\n",
    "\n",
    "# #Loading model and weights\n",
    "# json_file = open('nn.json','r')\n",
    "# nn_json = json_file.read()\n",
    "# json_file.close()\n",
    "# nn = model_from_json(nn_json)\n",
    "# nn.load_weights(weights_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize data\n",
    "x_train = x_train.astype('float32')\n",
    "x_valid = x_valid.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / 255\n",
    "x_valid = x_valid / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_valid = to_categorical(y_valid, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "#MNIST resolution\n",
    "img_rows, img_cols, channels = 32, 32, 3\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], channels, img_rows, img_cols)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], channels, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], channels, img_rows, img_cols)\n",
    "    input_shape = (channels, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channels)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], img_rows, img_cols, channels)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channels)\n",
    "    input_shape = (img_rows, img_cols, channels)\n",
    "    \n",
    "#Define the NN architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "#Two hidden layers\n",
    "nn = Sequential()\n",
    "nn.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Conv2D(32, (3,3), activation='relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(16, activation='relu'))\n",
    "nn.add(Dense(10, activation='softmax'))\n",
    "\n",
    "nn.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Starting tensorboard\n",
    "cbacks = []\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "cbacks.append(tensorboard)\n",
    "\n",
    "modfile = './model%d.h5' % int(time())\n",
    "mcheck = ModelCheckpoint(filepath=modfile, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "cbacks.append(mcheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.122143). Check your callbacks.\n",
      "test loss: 1.399267485385471\n",
      "test accuracy: 0.5140555555555556\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "history = nn.fit(x_train,y_train,batch_size=128,epochs=100, validation_data=(x_valid, y_valid), verbose=0, callbacks=cbacks)\n",
    "\n",
    "#Evaluate the model with test set\n",
    "score = nn.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.81      0.50      0.62      9000\n",
      "  automobile       0.77      0.36      0.49      9000\n",
      "        bird       0.54      0.47      0.50      9000\n",
      "         cat       0.44      0.36      0.40      9000\n",
      "        deer       0.33      0.71      0.45      9000\n",
      "         dog       0.40      0.44      0.42      9000\n",
      "        frog       0.87      0.47      0.61      9000\n",
      "       horse       0.61      0.63      0.62      9000\n",
      "        ship       0.84      0.36      0.51      9000\n",
      "       truck       0.43      0.83      0.57      9000\n",
      "\n",
      "   micro avg       0.51      0.51      0.51     90000\n",
      "   macro avg       0.60      0.51      0.52     90000\n",
      "weighted avg       0.60      0.51      0.52     90000\n",
      "\n",
      "[[4501  151  586  114  968  349   19  418  367 1527]\n",
      " [  63 3260  109  112  317  198   22  233   75 4611]\n",
      " [ 230   31 4243  583 2251  880  198  328   67  189]\n",
      " [  15   19  528 3284 2215 2125  198  361   15  240]\n",
      " [  41   22  367  451 6428  604   64  820   15  188]\n",
      " [  22   28  419  996 2429 3976   53  826   19  232]\n",
      " [   8   20  952 1367 1771  508 4192   77   14   91]\n",
      " [  16   22  149  223 1933  697    9 5665   11  275]\n",
      " [ 603  247  486  255 1010  434   67  283 3270 2345]\n",
      " [  63  424   79  100  296  193   12  334   53 7446]]\n"
     ]
    }
   ],
   "source": [
    "##Store Plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('1_cnn_accuracy.pdf')\n",
    "plt.close()\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('1_cnn_loss.pdf')\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "Y_pred = nn.predict(x_test)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print( 'Analysis of results' )\n",
    "target_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the NN architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "#Two hidden layers\n",
    "nn = Sequential()\n",
    "nn.add(Conv2D(64, (5,5), activation='relu', input_shape=input_shape))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Conv2D(32, (3,3), activation='relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(16, activation='relu'))\n",
    "nn.add(Dense(10, activation='softmax'))\n",
    "\n",
    "nn.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Starting tensorboard\n",
    "cbacks = []\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "cbacks.append(tensorboard)\n",
    "\n",
    "modfile = './model%d.h5' % int(time())\n",
    "mcheck = ModelCheckpoint(filepath=modfile, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "cbacks.append(mcheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.118654). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.118654). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.118654). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.118654). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.104277). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111423). Check your callbacks.\n",
      "test loss: 1.2614434773551093\n",
      "test accuracy: 0.5626111111111111\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "history = nn.fit(x_train,y_train,batch_size=128,epochs=100, validation_data=(x_valid, y_valid), verbose=0, callbacks=cbacks)\n",
    "\n",
    "#Evaluate the model with test set\n",
    "score = nn.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.72      0.69      9000\n",
      "           1       0.69      0.65      0.67      9000\n",
      "           2       0.61      0.39      0.48      9000\n",
      "           3       0.47      0.30      0.37      9000\n",
      "           4       0.37      0.65      0.47      9000\n",
      "           5       0.59      0.21      0.31      9000\n",
      "           6       0.53      0.83      0.65      9000\n",
      "           7       0.62      0.62      0.62      9000\n",
      "           8       0.57      0.73      0.64      9000\n",
      "           9       0.75      0.51      0.61      9000\n",
      "\n",
      "   micro avg       0.56      0.56      0.56     90000\n",
      "   macro avg       0.59      0.56      0.55     90000\n",
      "weighted avg       0.59      0.56      0.55     90000\n",
      "\n",
      "[[6505  139  250   44  352   24  115  149 1327   95]\n",
      " [ 525 5810   78   96  219   35  162  214  914  947]\n",
      " [ 689   57 3535  365 1752  221 1475  294  590   22]\n",
      " [ 126   84  405 2701 1961  561 2338  396  362   66]\n",
      " [ 215   30  359  348 5839  155  902  822  296   34]\n",
      " [ 173   66  453 1440 2465 1924 1125  988  292   74]\n",
      " [  71   25  359  266  560   55 7470   46  131   17]\n",
      " [ 213   63  146  253 2014  210  191 5613  201   96]\n",
      " [ 860  219  195   90  392   36  247  175 6608  178]\n",
      " [ 532 1931   52   90  293   43  131  363  935 4630]]\n"
     ]
    }
   ],
   "source": [
    "##Store Plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('2_cnn_accuracy.pdf')\n",
    "plt.close()\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('2_cnn_loss.pdf')\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "Y_pred = nn.predict(x_test)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print( 'Analysis of results' )\n",
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the NN architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Conv2D(32, (3, 3)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Dropout(0.25))\n",
    "\n",
    "nn.add(Conv2D(64, (3, 3), padding='same'))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Conv2D(64, (3, 3)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Dropout(0.25))\n",
    "\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(512))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Dropout(0.5))\n",
    "nn.add(Dense(10))\n",
    "nn.add(Activation('softmax'))\n",
    "\n",
    "nn.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Starting tensorboard\n",
    "cbacks = []\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "cbacks.append(tensorboard)\n",
    "\n",
    "modfile = './model%d.h5' % int(time())\n",
    "mcheck = ModelCheckpoint(filepath=modfile, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "cbacks.append(mcheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.117344). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.107362). Check your callbacks.\n",
      "test loss: 0.9602733841896057\n",
      "test accuracy: 0.6569\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "history = nn.fit(x_train,y_train,batch_size=128,epochs=100, validation_data=(x_valid, y_valid), verbose=0, callbacks=cbacks)\n",
    "\n",
    "#Evaluate the model with test set\n",
    "score = nn.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76      9000\n",
      "           1       0.76      0.69      0.72      9000\n",
      "           2       0.64      0.57      0.60      9000\n",
      "           3       0.48      0.53      0.50      9000\n",
      "           4       0.63      0.49      0.55      9000\n",
      "           5       0.51      0.47      0.49      9000\n",
      "           6       0.74      0.78      0.76      9000\n",
      "           7       0.73      0.72      0.72      9000\n",
      "           8       0.68      0.78      0.73      9000\n",
      "           9       0.65      0.78      0.71      9000\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     90000\n",
      "   macro avg       0.66      0.66      0.65     90000\n",
      "weighted avg       0.66      0.66      0.65     90000\n",
      "\n",
      "[[6810  198  228   96   61   61   47  111  998  390]\n",
      " [ 229 6184   46   57   17   59   30   66  332 1980]\n",
      " [ 587   51 5153  735  457  508  794  167  463   85]\n",
      " [ 100   79  507 4791  533 1571  813  199  231  176]\n",
      " [ 188   52  626 1018 4450  843  423  966  310  124]\n",
      " [ 110  119  551 1869  773 4235  266  645  250  182]\n",
      " [  54   43  520  803  146  234 7038   19  107   36]\n",
      " [ 132   82  202  403  523  725   37 6441  172  283]\n",
      " [ 583  288  197  147   83   84   81   82 6980  475]\n",
      " [ 203 1057   42   94   27   53   21  114  350 7039]]\n"
     ]
    }
   ],
   "source": [
    "##Store Plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('3_cnn_accuracy.pdf')\n",
    "plt.close()\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('3_cnn_loss.pdf')\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "Y_pred = nn.predict(x_test)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print( 'Analysis of results' )\n",
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional 4\n",
    "without regularizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the NN architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Conv2D(32, (3, 3)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "nn.add(Conv2D(64, (3, 3), padding='same'))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Conv2D(64, (3, 3)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(512))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Dense(10))\n",
    "nn.add(Activation('softmax'))\n",
    "\n",
    "nn.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Starting tensorboard\n",
    "cbacks = []\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "cbacks.append(tensorboard)\n",
    "\n",
    "modfile = './model%d.h5' % int(time())\n",
    "mcheck = ModelCheckpoint(filepath=modfile, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "cbacks.append(mcheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7f9d5b189bf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Start training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Evaluate the model with test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\builds\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\builds\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m                 verbose=0)\n\u001b[0m\u001b[0;32m    234\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m               \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\builds\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\builds\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2986\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\builds\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "history = nn.fit(x_train,y_train,batch_size=128,epochs=100, validation_data=(x_valid, y_valid), verbose=0, callbacks=cbacks)\n",
    "\n",
    "#Evaluate the model with test set\n",
    "score = nn.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Store Plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('4_cnn_accuracy.pdf')\n",
    "plt.close()\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('4_cnn_loss.pdf')\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "Y_pred = nn.predict(x_test)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print( 'Analysis of results' )\n",
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional 5\n",
    "with different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the NN architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Conv2D(32, (3, 3)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "nn.add(Conv2D(64, (3, 3), padding='same'))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Conv2D(64, (3, 3)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(512))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Dense(10))\n",
    "nn.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "nn.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Starting tensorboard\n",
    "cbacks = []\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "cbacks.append(tensorboard)\n",
    "\n",
    "modfile = './model%d.h5' % int(time())\n",
    "mcheck = ModelCheckpoint(filepath=modfile, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "cbacks.append(mcheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start training\n",
    "history = nn.fit(x_train,y_train,batch_size=128,epochs=100, validation_data=(x_valid, y_valid), verbose=0, callbacks=cbacks)\n",
    "\n",
    "#Evaluate the model with test set\n",
    "score = nn.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Store Plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('5_cnn_accuracy.pdf')\n",
    "plt.close()\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('5_cnn_loss.pdf')\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "Y_pred = nn.predict(x_test)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print( 'Analysis of results' )\n",
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "builds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
