{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version 2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from tensorflow import keras\n",
    "print( 'Using Keras version', keras.__version__)\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "x_valid = np.load('x_valid.npy')\n",
    "y_valid = np.load('y_valid.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "#Normalize data\n",
    "x_train = x_train.astype('float32')\n",
    "x_valid = x_valid.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / 255\n",
    "x_valid = x_valid / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_valid = to_categorical(y_valid, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "#MNIST resolution\n",
    "img_rows, img_cols, channels = 32, 32, 3\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], channels, img_rows, img_cols)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], channels, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], channels, img_rows, img_cols)\n",
    "    input_shape = (channels, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channels)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], img_rows, img_cols, channels)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channels)\n",
    "    input_shape = (img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "classes = sorted(os.walk('./train/').__next__()[1])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples 90000\n",
      "Size of train examples (32, 32, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                196672    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 200,138\n",
      "Trainable params: 200,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the NN architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "#Two hidden layers\n",
    "nn = Sequential()\n",
    "nn.add(Dense(64,activation='relu',input_shape=(3072,)))\n",
    "nn.add(Dense(32,activation='relu'))\n",
    "nn.add(Dense(32,activation='relu'))\n",
    "nn.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#Model visualization\n",
    "#We can plot the model by using the ```plot_model``` function. We need to install *pydot, graphviz and pydot-ng*.\n",
    "#from keras.util import plot_model\n",
    "#plot_model(nn, to_file='nn.png', show_shapes=true)\n",
    "\n",
    "#Compile the NN\n",
    "nn.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Starting tensorboard\n",
    "cbacks = []\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "cbacks.append(tensorboard)\n",
    "\n",
    "modfile = './model%d.h5' % int(time())\n",
    "mcheck = ModelCheckpoint(filepath=modfile, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "cbacks.append(mcheck)\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "cbacks.append(early)\n",
    "\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start training\n",
    "history = nn.fit(x_train,y_train,batch_size=128,epochs=20, verbose=1, callbacks=cbacks)\n",
    "\n",
    "#Evaluate the model with test set\n",
    "score = nn.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test accuracy:', score[1])\n",
    "\n",
    "##Store Plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig('mnist_fnn_accuracy.pdf')\n",
    "plt.close()\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig('mnist_fnn_loss.pdf')\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "Y_pred = nn.predict(x_test)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print( 'Analysis of results' )\n",
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
    "\n",
    "# #Saving model and weights\n",
    "# from keras.models import model_from_json\n",
    "# nn_json = nn.to_json()\n",
    "# with open('nn.json', 'w') as json_file:\n",
    "#         json_file.write(nn_json)\n",
    "# weights_file = \"weights-MNIST_\"+str(score[1])+\".hdf5\"\n",
    "# nn.save_weights(weights_file,overwrite=True)\n",
    "\n",
    "# #Loading model and weights\n",
    "# json_file = open('nn.json','r')\n",
    "# nn_json = json_file.read()\n",
    "# json_file.close()\n",
    "# nn = model_from_json(nn_json)\n",
    "# nn.load_weights(weights_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                18448     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 38,874\n",
      "Trainable params: 38,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the NN architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "#Two hidden layers\n",
    "nn = Sequential()\n",
    "nn.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Conv2D(32, (3,3), activation='relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(16, activation='relu'))\n",
    "nn.add(Dense(10, activation='softmax'))\n",
    "\n",
    "nn.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Starting tensorboard\n",
    "cbacks = []\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "cbacks.append(tensorboard)\n",
    "\n",
    "modfile = './model%d.h5' % int(time())\n",
    "mcheck = ModelCheckpoint(filepath=modfile, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "cbacks.append(mcheck)\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "cbacks.append(early)\n",
    "\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.122143). Check your callbacks.\n",
      "test loss: 1.399267485385471\n",
      "test accuracy: 0.5140555555555556\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "history = nn.fit(x_train,y_train,batch_size=128,epochs=1000, validation_data=(x_valid, y_valid), verbose=0, callbacks=cbacks)\n",
    "\n",
    "#Evaluate the model with test set\n",
    "score = nn.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.81      0.50      0.62      9000\n",
      "  automobile       0.77      0.36      0.49      9000\n",
      "        bird       0.54      0.47      0.50      9000\n",
      "         cat       0.44      0.36      0.40      9000\n",
      "        deer       0.33      0.71      0.45      9000\n",
      "         dog       0.40      0.44      0.42      9000\n",
      "        frog       0.87      0.47      0.61      9000\n",
      "       horse       0.61      0.63      0.62      9000\n",
      "        ship       0.84      0.36      0.51      9000\n",
      "       truck       0.43      0.83      0.57      9000\n",
      "\n",
      "   micro avg       0.51      0.51      0.51     90000\n",
      "   macro avg       0.60      0.51      0.52     90000\n",
      "weighted avg       0.60      0.51      0.52     90000\n",
      "\n",
      "[[4501  151  586  114  968  349   19  418  367 1527]\n",
      " [  63 3260  109  112  317  198   22  233   75 4611]\n",
      " [ 230   31 4243  583 2251  880  198  328   67  189]\n",
      " [  15   19  528 3284 2215 2125  198  361   15  240]\n",
      " [  41   22  367  451 6428  604   64  820   15  188]\n",
      " [  22   28  419  996 2429 3976   53  826   19  232]\n",
      " [   8   20  952 1367 1771  508 4192   77   14   91]\n",
      " [  16   22  149  223 1933  697    9 5665   11  275]\n",
      " [ 603  247  486  255 1010  434   67  283 3270 2345]\n",
      " [  63  424   79  100  296  193   12  334   53 7446]]\n"
     ]
    }
   ],
   "source": [
    "##Store Plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('1_cnn_accuracy.pdf')\n",
    "plt.close()\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('1_cnn_loss.pdf')\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "Y_pred = nn.predict(x_test)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print( 'Analysis of results' )\n",
    "target_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 64)        4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                18448     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 41,946\n",
      "Trainable params: 41,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the NN architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "#Two hidden layers\n",
    "nn = Sequential()\n",
    "nn.add(Conv2D(64, (5,5), activation='relu', input_shape=input_shape))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Conv2D(32, (3,3), activation='relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(16, activation='relu'))\n",
    "nn.add(Dense(10, activation='softmax'))\n",
    "\n",
    "nn.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Starting tensorboard\n",
    "cbacks = []\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "cbacks.append(tensorboard)\n",
    "\n",
    "modfile = './model%d.h5' % int(time())\n",
    "mcheck = ModelCheckpoint(filepath=modfile, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "cbacks.append(mcheck)\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "cbacks.append(early)\n",
    "\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.118654). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.118654). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.118654). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.118654). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.104277). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111423). Check your callbacks.\n",
      "test loss: 1.2614434773551093\n",
      "test accuracy: 0.5626111111111111\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "history = nn.fit(x_train,y_train,batch_size=128,epochs=1000, validation_data=(x_valid, y_valid), verbose=0, callbacks=cbacks)\n",
    "\n",
    "#Evaluate the model with test set\n",
    "score = nn.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.72      0.69      9000\n",
      "           1       0.69      0.65      0.67      9000\n",
      "           2       0.61      0.39      0.48      9000\n",
      "           3       0.47      0.30      0.37      9000\n",
      "           4       0.37      0.65      0.47      9000\n",
      "           5       0.59      0.21      0.31      9000\n",
      "           6       0.53      0.83      0.65      9000\n",
      "           7       0.62      0.62      0.62      9000\n",
      "           8       0.57      0.73      0.64      9000\n",
      "           9       0.75      0.51      0.61      9000\n",
      "\n",
      "   micro avg       0.56      0.56      0.56     90000\n",
      "   macro avg       0.59      0.56      0.55     90000\n",
      "weighted avg       0.59      0.56      0.55     90000\n",
      "\n",
      "[[6505  139  250   44  352   24  115  149 1327   95]\n",
      " [ 525 5810   78   96  219   35  162  214  914  947]\n",
      " [ 689   57 3535  365 1752  221 1475  294  590   22]\n",
      " [ 126   84  405 2701 1961  561 2338  396  362   66]\n",
      " [ 215   30  359  348 5839  155  902  822  296   34]\n",
      " [ 173   66  453 1440 2465 1924 1125  988  292   74]\n",
      " [  71   25  359  266  560   55 7470   46  131   17]\n",
      " [ 213   63  146  253 2014  210  191 5613  201   96]\n",
      " [ 860  219  195   90  392   36  247  175 6608  178]\n",
      " [ 532 1931   52   90  293   43  131  363  935 4630]]\n"
     ]
    }
   ],
   "source": [
    "##Store Plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('2_cnn_accuracy.pdf')\n",
    "plt.close()\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('2_cnn_loss.pdf')\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "Y_pred = nn.predict(x_test)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print( 'Analysis of results' )\n",
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the NN architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Conv2D(32, (3, 3)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Dropout(0.25))\n",
    "\n",
    "nn.add(Conv2D(64, (3, 3), padding='same'))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Conv2D(64, (3, 3)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Dropout(0.25))\n",
    "\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(512))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Dropout(0.5))\n",
    "nn.add(Dense(10))\n",
    "nn.add(Activation('softmax'))\n",
    "\n",
    "nn.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Starting tensorboard\n",
    "cbacks = []\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "cbacks.append(tensorboard)\n",
    "\n",
    "modfile = './model%d.h5' % int(time())\n",
    "mcheck = ModelCheckpoint(filepath=modfile, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "cbacks.append(mcheck)\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "cbacks.append(early)\n",
    "\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.117344). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.107362). Check your callbacks.\n",
      "test loss: 0.9602733841896057\n",
      "test accuracy: 0.6569\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "history = nn.fit(x_train,y_train,batch_size=128,epochs=1000, validation_data=(x_valid, y_valid), verbose=0, callbacks=cbacks)\n",
    "\n",
    "#Evaluate the model with test set\n",
    "score = nn.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76      9000\n",
      "           1       0.76      0.69      0.72      9000\n",
      "           2       0.64      0.57      0.60      9000\n",
      "           3       0.48      0.53      0.50      9000\n",
      "           4       0.63      0.49      0.55      9000\n",
      "           5       0.51      0.47      0.49      9000\n",
      "           6       0.74      0.78      0.76      9000\n",
      "           7       0.73      0.72      0.72      9000\n",
      "           8       0.68      0.78      0.73      9000\n",
      "           9       0.65      0.78      0.71      9000\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     90000\n",
      "   macro avg       0.66      0.66      0.65     90000\n",
      "weighted avg       0.66      0.66      0.65     90000\n",
      "\n",
      "[[6810  198  228   96   61   61   47  111  998  390]\n",
      " [ 229 6184   46   57   17   59   30   66  332 1980]\n",
      " [ 587   51 5153  735  457  508  794  167  463   85]\n",
      " [ 100   79  507 4791  533 1571  813  199  231  176]\n",
      " [ 188   52  626 1018 4450  843  423  966  310  124]\n",
      " [ 110  119  551 1869  773 4235  266  645  250  182]\n",
      " [  54   43  520  803  146  234 7038   19  107   36]\n",
      " [ 132   82  202  403  523  725   37 6441  172  283]\n",
      " [ 583  288  197  147   83   84   81   82 6980  475]\n",
      " [ 203 1057   42   94   27   53   21  114  350 7039]]\n"
     ]
    }
   ],
   "source": [
    "##Store Plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('3_cnn_accuracy.pdf')\n",
    "plt.close()\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('3_cnn_loss.pdf')\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "Y_pred = nn.predict(x_test)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print( 'Analysis of results' )\n",
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional 4\n",
    "without regularizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the NN architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Conv2D(32, (3, 3)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "nn.add(Conv2D(64, (3, 3), padding='same'))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Conv2D(64, (3, 3)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(512))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Dense(10))\n",
    "nn.add(Activation('softmax'))\n",
    "\n",
    "nn.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Starting tensorboard\n",
    "cbacks = []\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "cbacks.append(tensorboard)\n",
    "\n",
    "modfile = './model%d.h5' % int(time())\n",
    "mcheck = ModelCheckpoint(filepath=modfile, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "cbacks.append(mcheck)\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "cbacks.append(early)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7f9d5b189bf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Start training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Evaluate the model with test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\builds\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\builds\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m                 verbose=0)\n\u001b[0m\u001b[0;32m    234\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m               \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\builds\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\builds\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2986\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\builds\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "history = nn.fit(x_train,y_train,batch_size=128,epochs=1000, validation_data=(x_valid, y_valid), verbose=0, callbacks=cbacks)\n",
    "\n",
    "#Evaluate the model with test set\n",
    "score = nn.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Store Plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('4_cnn_accuracy.pdf')\n",
    "plt.close()\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('4_cnn_loss.pdf')\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "Y_pred = nn.predict(x_test)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print( 'Analysis of results' )\n",
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional 5\n",
    "with different optimizers -ADAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the NN architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Conv2D(32, (3, 3)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Dropout(0.25))\n",
    "\n",
    "nn.add(Conv2D(64, (3, 3), padding='same'))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Conv2D(64, (3, 3)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Dropout(0.25))\n",
    "\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(512))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Dropout(0.5))\n",
    "nn.add(Dense(10))\n",
    "nn.add(Activation('softmax'))\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "nn.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Starting tensorboard\n",
    "cbacks = []\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "cbacks.append(tensorboard)\n",
    "\n",
    "modfile = './model%d.h5' % int(time())\n",
    "mcheck = ModelCheckpoint(filepath=modfile, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "cbacks.append(mcheck)\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "cbacks.append(early)\n",
    "\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.197420). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.129001). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.151603). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.129001). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.122680). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.116359). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.110506). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.104652). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.106726). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.104183). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100999). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.121945). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.113239). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.113239). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.110059). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100813). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100813). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111946). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100813). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.113570). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100813). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.108295). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.110956). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.112280). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.112280). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.112280). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.112280). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.112280). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.117455). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.110011). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.110011). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.110011). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.103610). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.107578). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100859). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100859). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.105411). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111781). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111781). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111781). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111781). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.108298). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.101990). Check your callbacks.\n",
      "test loss: 0.9499173873583475\n",
      "test accuracy: 0.6715666666666666\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "history = nn.fit(x_train,y_train,batch_size=128,epochs=1000, validation_data=(x_valid, y_valid), verbose=0, callbacks=cbacks)\n",
    "\n",
    "#Evaluate the model with test set\n",
    "score = nn.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.78      9000\n",
      "           1       0.76      0.74      0.75      9000\n",
      "           2       0.70      0.54      0.61      9000\n",
      "           3       0.54      0.48      0.51      9000\n",
      "           4       0.52      0.69      0.59      9000\n",
      "           5       0.53      0.51      0.52      9000\n",
      "           6       0.67      0.84      0.75      9000\n",
      "           7       0.78      0.69      0.73      9000\n",
      "           8       0.71      0.78      0.74      9000\n",
      "           9       0.76      0.69      0.73      9000\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     90000\n",
      "   macro avg       0.68      0.67      0.67     90000\n",
      "weighted avg       0.68      0.67      0.67     90000\n",
      "\n",
      "[[6770  139  363  101  156   88   97  166  940  180]\n",
      " [ 173 6700   55   45   74  102   83  100  476 1192]\n",
      " [ 356   37 4883  596 1035  563 1098  111  291   30]\n",
      " [  65   45  315 4342 1144 1465 1267  143  152   62]\n",
      " [ 105   24  325  606 6181  667  360  541  157   34]\n",
      " [  77   63  299 1356 1457 4563  497  442  182   64]\n",
      " [  19   19  341  421  320  207 7564   18   82    9]\n",
      " [  97   63  139  316 1162  732   78 6166  150   97]\n",
      " [ 515  215  248  141  192  137  145  107 7059  241]\n",
      " [ 206 1548   41   95  108   99   55  147  488 6213]]\n"
     ]
    }
   ],
   "source": [
    "##Store Plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('5_cnn_accuracy.pdf')\n",
    "plt.close()\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('5_cnn_loss.pdf')\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "Y_pred = nn.predict(x_test)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print( 'Analysis of results' )\n",
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional 6\n",
    "with RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the NN architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Conv2D(32, (3, 3)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Dropout(0.25))\n",
    "\n",
    "nn.add(Conv2D(64, (3, 3), padding='same'))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Conv2D(64, (3, 3)))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nn.add(Dropout(0.25))\n",
    "\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(512))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Dropout(0.5))\n",
    "nn.add(Dense(10))\n",
    "nn.add(Activation('softmax'))\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "opt = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "nn.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Starting tensorboard\n",
    "cbacks = []\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "cbacks.append(tensorboard)\n",
    "\n",
    "modfile = './model%d.h5' % int(time())\n",
    "mcheck = ModelCheckpoint(filepath=modfile, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "cbacks.append(mcheck)\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "cbacks.append(early)\n",
    "\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples, validate on 90000 samples\n",
      "Epoch 1/1000\n",
      "  256/90000 [..............................] - ETA: 27:55 - loss: 2.4408 - acc: 0.1094WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.278503). Check your callbacks.\n",
      " 7424/90000 [=>............................] - ETA: 1:51 - loss: 2.2208 - acc: 0.1782WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.102948). Check your callbacks.\n",
      " 7552/90000 [=>............................] - ETA: 1:51 - loss: 2.2182 - acc: 0.1789WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.105309). Check your callbacks.\n",
      " 7680/90000 [=>............................] - ETA: 1:51 - loss: 2.2146 - acc: 0.1802WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.105309). Check your callbacks.\n",
      " 7808/90000 [=>............................] - ETA: 1:52 - loss: 2.2119 - acc: 0.1825WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.110570). Check your callbacks.\n",
      " 7936/90000 [=>............................] - ETA: 1:52 - loss: 2.2086 - acc: 0.1841WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.116741). Check your callbacks.\n",
      " 8064/90000 [=>............................] - ETA: 1:53 - loss: 2.2058 - acc: 0.1849WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.116741). Check your callbacks.\n",
      " 8192/90000 [=>............................] - ETA: 1:53 - loss: 2.2028 - acc: 0.1859WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.116741). Check your callbacks.\n",
      " 8320/90000 [=>............................] - ETA: 1:55 - loss: 2.2000 - acc: 0.1865WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.116741). Check your callbacks.\n",
      " 8448/90000 [=>............................] - ETA: 1:54 - loss: 2.1976 - acc: 0.1871WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.116741). Check your callbacks.\n",
      " 8576/90000 [=>............................] - ETA: 1:54 - loss: 2.1952 - acc: 0.1880WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.131298). Check your callbacks.\n",
      " 8704/90000 [=>............................] - ETA: 1:54 - loss: 2.1937 - acc: 0.1881WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.149687). Check your callbacks.\n",
      " 8832/90000 [=>............................] - ETA: 1:54 - loss: 2.1976 - acc: 0.1884WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.149687). Check your callbacks.\n",
      " 8960/90000 [=>............................] - ETA: 1:54 - loss: 2.1956 - acc: 0.1890WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.155856). Check your callbacks.\n",
      " 9088/90000 [==>...........................] - ETA: 1:53 - loss: 2.1932 - acc: 0.1903WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.155856). Check your callbacks.\n",
      " 9216/90000 [==>...........................] - ETA: 1:53 - loss: 2.1902 - acc: 0.1918WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.155856). Check your callbacks.\n",
      " 9344/90000 [==>...........................] - ETA: 1:54 - loss: 2.1887 - acc: 0.1918WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.148242). Check your callbacks.\n",
      " 9472/90000 [==>...........................] - ETA: 1:55 - loss: 2.1859 - acc: 0.1926WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.142451). Check your callbacks.\n",
      " 9600/90000 [==>...........................] - ETA: 1:54 - loss: 2.1831 - acc: 0.1941WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.138208). Check your callbacks.\n",
      " 9728/90000 [==>...........................] - ETA: 1:53 - loss: 2.1797 - acc: 0.1954WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.138208). Check your callbacks.\n",
      " 9856/90000 [==>...........................] - ETA: 1:53 - loss: 2.1794 - acc: 0.1952WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.138208). Check your callbacks.\n",
      " 9984/90000 [==>...........................] - ETA: 1:55 - loss: 2.1781 - acc: 0.1942WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.138208). Check your callbacks.\n",
      "10112/90000 [==>...........................] - ETA: 1:54 - loss: 2.1870 - acc: 0.1941WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.130032). Check your callbacks.\n",
      "10240/90000 [==>...........................] - ETA: 1:54 - loss: 2.1861 - acc: 0.1948WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.130032). Check your callbacks.\n",
      "10368/90000 [==>...........................] - ETA: 1:54 - loss: 2.1834 - acc: 0.1955WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.139835). Check your callbacks.\n",
      "10496/90000 [==>...........................] - ETA: 1:53 - loss: 2.1803 - acc: 0.1966WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.139835). Check your callbacks.\n",
      "10624/90000 [==>...........................] - ETA: 1:53 - loss: 2.1776 - acc: 0.1981WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.129213). Check your callbacks.\n",
      "10752/90000 [==>...........................] - ETA: 1:53 - loss: 2.1744 - acc: 0.1996WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.130144). Check your callbacks.\n",
      "10880/90000 [==>...........................] - ETA: 1:52 - loss: 2.1719 - acc: 0.2005WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.145842). Check your callbacks.\n",
      "11008/90000 [==>...........................] - ETA: 1:53 - loss: 2.1705 - acc: 0.2010WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.146019). Check your callbacks.\n",
      "11136/90000 [==>...........................] - ETA: 1:52 - loss: 2.1671 - acc: 0.2024WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.145842). Check your callbacks.\n",
      "11264/90000 [==>...........................] - ETA: 1:52 - loss: 2.1643 - acc: 0.2031WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.135958). Check your callbacks.\n",
      "11392/90000 [==>...........................] - ETA: 1:51 - loss: 2.1612 - acc: 0.2041WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.135958). Check your callbacks.\n",
      "11520/90000 [==>...........................] - ETA: 1:52 - loss: 2.1601 - acc: 0.2051WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.135958). Check your callbacks.\n",
      "11648/90000 [==>...........................] - ETA: 1:52 - loss: 2.1629 - acc: 0.2045WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.135958). Check your callbacks.\n",
      "11776/90000 [==>...........................] - ETA: 1:51 - loss: 2.1635 - acc: 0.2036WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.135958). Check your callbacks.\n",
      "11904/90000 [==>...........................] - ETA: 1:51 - loss: 2.1628 - acc: 0.2040WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.136416). Check your callbacks.\n",
      "12032/90000 [===>..........................] - ETA: 1:51 - loss: 2.1606 - acc: 0.2045WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.136416). Check your callbacks.\n",
      "12160/90000 [===>..........................] - ETA: 1:51 - loss: 2.1584 - acc: 0.2059WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.126633). Check your callbacks.\n",
      "12288/90000 [===>..........................] - ETA: 1:52 - loss: 2.1554 - acc: 0.2074WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.126633). Check your callbacks.\n",
      "12416/90000 [===>..........................] - ETA: 1:53 - loss: 2.1527 - acc: 0.2089WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.139293). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12544/90000 [===>..........................] - ETA: 1:54 - loss: 2.1546 - acc: 0.2092WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.160445). Check your callbacks.\n",
      "12672/90000 [===>..........................] - ETA: 1:56 - loss: 2.1546 - acc: 0.2094WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.180249). Check your callbacks.\n",
      "12800/90000 [===>..........................] - ETA: 1:56 - loss: 2.1528 - acc: 0.2100WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.177757). Check your callbacks.\n",
      "12928/90000 [===>..........................] - ETA: 1:57 - loss: 2.1506 - acc: 0.2108WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.193168). Check your callbacks.\n",
      "13056/90000 [===>..........................] - ETA: 1:58 - loss: 2.1498 - acc: 0.2107WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.193168). Check your callbacks.\n",
      "13184/90000 [===>..........................] - ETA: 1:58 - loss: 2.1473 - acc: 0.2112WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.193168). Check your callbacks.\n",
      "13312/90000 [===>..........................] - ETA: 1:58 - loss: 2.1457 - acc: 0.2117WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.179821). Check your callbacks.\n",
      "13440/90000 [===>..........................] - ETA: 1:57 - loss: 2.1434 - acc: 0.2124WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.179821). Check your callbacks.\n",
      "13568/90000 [===>..........................] - ETA: 1:57 - loss: 2.1409 - acc: 0.2129WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.147865). Check your callbacks.\n",
      "13696/90000 [===>..........................] - ETA: 1:57 - loss: 2.1384 - acc: 0.2136WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.127908). Check your callbacks.\n",
      "13824/90000 [===>..........................] - ETA: 1:57 - loss: 2.1353 - acc: 0.2152WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.110901). Check your callbacks.\n",
      "15104/90000 [====>.........................] - ETA: 2:03 - loss: 2.1264 - acc: 0.2201WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.116030). Check your callbacks.\n",
      "15232/90000 [====>.........................] - ETA: 2:03 - loss: 2.1248 - acc: 0.2201WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.116030). Check your callbacks.\n",
      "15360/90000 [====>.........................] - ETA: 2:03 - loss: 2.1236 - acc: 0.2200WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.108784). Check your callbacks.\n",
      "15488/90000 [====>.........................] - ETA: 2:03 - loss: 2.1239 - acc: 0.2200WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.101300). Check your callbacks.\n",
      "34944/90000 [==========>...................] - ETA: 4:06 - loss: 1.9822 - acc: 0.2736WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.109049). Check your callbacks.\n",
      "35072/90000 [==========>...................] - ETA: 4:09 - loss: 1.9812 - acc: 0.2738WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100105). Check your callbacks.\n",
      "35200/90000 [==========>...................] - ETA: 4:13 - loss: 1.9802 - acc: 0.2743WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100105). Check your callbacks.\n",
      "35328/90000 [==========>...................] - ETA: 4:17 - loss: 1.9792 - acc: 0.2747WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100387). Check your callbacks.\n",
      "35456/90000 [==========>...................] - ETA: 4:21 - loss: 1.9784 - acc: 0.2748WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.112723). Check your callbacks.\n",
      "35584/90000 [==========>...................] - ETA: 4:24 - loss: 1.9775 - acc: 0.2750WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.112723). Check your callbacks.\n",
      "35712/90000 [==========>...................] - ETA: 4:27 - loss: 1.9772 - acc: 0.2750WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.112723). Check your callbacks.\n",
      "35840/90000 [==========>...................] - ETA: 4:29 - loss: 1.9765 - acc: 0.2752WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.112723). Check your callbacks.\n",
      "35968/90000 [==========>...................] - ETA: 4:32 - loss: 1.9756 - acc: 0.2756WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.104952). Check your callbacks.\n",
      "36864/90000 [===========>..................] - ETA: 4:50 - loss: 1.9695 - acc: 0.2781WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.102759). Check your callbacks.\n",
      "36992/90000 [===========>..................] - ETA: 4:55 - loss: 1.9690 - acc: 0.2783WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.102759). Check your callbacks.\n",
      "37120/90000 [===========>..................] - ETA: 4:57 - loss: 1.9688 - acc: 0.2785WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.114732). Check your callbacks.\n",
      "37248/90000 [===========>..................] - ETA: 4:59 - loss: 1.9695 - acc: 0.2784WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.114732). Check your callbacks.\n",
      "37376/90000 [===========>..................] - ETA: 5:02 - loss: 1.9691 - acc: 0.2784WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.124771). Check your callbacks.\n",
      "37504/90000 [===========>..................] - ETA: 5:05 - loss: 1.9684 - acc: 0.2786WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.124771). Check your callbacks.\n",
      "37632/90000 [===========>..................] - ETA: 5:07 - loss: 1.9677 - acc: 0.2790WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.124771). Check your callbacks.\n",
      "37760/90000 [===========>..................] - ETA: 5:09 - loss: 1.9665 - acc: 0.2795WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.113568). Check your callbacks.\n",
      "73216/90000 [=======================>......] - ETA: 1:19 - loss: 1.8185 - acc: 0.3313- ETA: 2: - ETA: 2:16 WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111480). Check your callbacks.\n",
      "73344/90000 [=======================>......] - ETA: 1:19 - loss: 1.8180 - acc: 0.3314WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.123272). Check your callbacks.\n",
      "73472/90000 [=======================>......] - ETA: 1:18 - loss: 1.8174 - acc: 0.3316WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.134862). Check your callbacks.\n",
      "73600/90000 [=======================>......] - ETA: 1:17 - loss: 1.8169 - acc: 0.3317WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.134862). Check your callbacks.\n",
      "73728/90000 [=======================>......] - ETA: 1:17 - loss: 1.8166 - acc: 0.3319WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.135945). Check your callbacks.\n",
      "73856/90000 [=======================>......] - ETA: 1:16 - loss: 1.8161 - acc: 0.3321WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.143860). Check your callbacks.\n",
      "73984/90000 [=======================>......] - ETA: 1:15 - loss: 1.8158 - acc: 0.3322WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.143860). Check your callbacks.\n",
      "74112/90000 [=======================>......] - ETA: 1:15 - loss: 1.8158 - acc: 0.3322WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.143860). Check your callbacks.\n",
      "74240/90000 [=======================>......] - ETA: 1:14 - loss: 1.8151 - acc: 0.3325WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.135945). Check your callbacks.\n",
      "74368/90000 [=======================>......] - ETA: 1:13 - loss: 1.8145 - acc: 0.3326WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.129055). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74496/90000 [=======================>......] - ETA: 1:12 - loss: 1.8142 - acc: 0.3327WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.121879). Check your callbacks.\n",
      "74624/90000 [=======================>......] - ETA: 1:12 - loss: 1.8136 - acc: 0.3329WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111393). Check your callbacks.\n",
      "79744/90000 [=========================>....] - ETA: 45s - loss: 1.7990 - acc: 0.3381 ETA: 56WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100744). Check your callbacks.\n",
      "79872/90000 [=========================>....] - ETA: 45s - loss: 1.7988 - acc: 0.3382WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100744). Check your callbacks.\n",
      "80000/90000 [=========================>....] - ETA: 44s - loss: 1.7986 - acc: 0.3383WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.109764). Check your callbacks.\n",
      "80128/90000 [=========================>....] - ETA: 43s - loss: 1.7983 - acc: 0.3384WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.109764). Check your callbacks.\n",
      "80256/90000 [=========================>....] - ETA: 43s - loss: 1.7978 - acc: 0.3386WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.109764). Check your callbacks.\n",
      "80384/90000 [=========================>....] - ETA: 42s - loss: 1.7973 - acc: 0.3387WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.109764). Check your callbacks.\n",
      "80512/90000 [=========================>....] - ETA: 41s - loss: 1.7970 - acc: 0.3388WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.120090). Check your callbacks.\n",
      "80640/90000 [=========================>....] - ETA: 41s - loss: 1.7965 - acc: 0.3390WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111374). Check your callbacks.\n",
      "80768/90000 [=========================>....] - ETA: 40s - loss: 1.7960 - acc: 0.3392WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.128019). Check your callbacks.\n",
      "80896/90000 [=========================>....] - ETA: 40s - loss: 1.7956 - acc: 0.3393WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.123592). Check your callbacks.\n",
      "81024/90000 [==========================>...] - ETA: 39s - loss: 1.7953 - acc: 0.3394WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.121776). Check your callbacks.\n",
      "81152/90000 [==========================>...] - ETA: 38s - loss: 1.7949 - acc: 0.3394WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.121776). Check your callbacks.\n",
      "81280/90000 [==========================>...] - ETA: 38s - loss: 1.7950 - acc: 0.3394WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.112904). Check your callbacks.\n",
      "81408/90000 [==========================>...] - ETA: 37s - loss: 1.7945 - acc: 0.3395WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.112904). Check your callbacks.\n",
      "81536/90000 [==========================>...] - ETA: 37s - loss: 1.7941 - acc: 0.3397WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.102962). Check your callbacks.\n",
      "81664/90000 [==========================>...] - ETA: 36s - loss: 1.7937 - acc: 0.3399WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.112904). Check your callbacks.\n",
      "81792/90000 [==========================>...] - ETA: 35s - loss: 1.7932 - acc: 0.3401WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.112904). Check your callbacks.\n",
      "81920/90000 [==========================>...] - ETA: 35s - loss: 1.7932 - acc: 0.3402WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.112904). Check your callbacks.\n",
      "82048/90000 [==========================>...] - ETA: 34s - loss: 1.7929 - acc: 0.3403WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.102962). Check your callbacks.\n",
      "82176/90000 [==========================>...] - ETA: 34s - loss: 1.7927 - acc: 0.3405WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.101775). Check your callbacks.\n",
      "82304/90000 [==========================>...] - ETA: 33s - loss: 1.7923 - acc: 0.3406WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.101775). Check your callbacks.\n",
      "82432/90000 [==========================>...] - ETA: 32s - loss: 1.7918 - acc: 0.3408WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.101608). Check your callbacks.\n",
      "83456/90000 [==========================>...] - ETA: 28s - loss: 1.7882 - acc: 0.3421WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.108719). Check your callbacks.\n",
      "83584/90000 [==========================>...] - ETA: 27s - loss: 1.7876 - acc: 0.3425WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.108719). Check your callbacks.\n",
      "83712/90000 [==========================>...] - ETA: 26s - loss: 1.7871 - acc: 0.3427WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111443). Check your callbacks.\n",
      "83840/90000 [==========================>...] - ETA: 26s - loss: 1.7867 - acc: 0.3429WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111443). Check your callbacks.\n",
      "83968/90000 [==========================>...] - ETA: 25s - loss: 1.7862 - acc: 0.3430WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111443). Check your callbacks.\n",
      "84096/90000 [===========================>..] - ETA: 25s - loss: 1.7857 - acc: 0.3431WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.109516). Check your callbacks.\n",
      "84224/90000 [===========================>..] - ETA: 24s - loss: 1.7853 - acc: 0.3433WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.107905). Check your callbacks.\n",
      "84352/90000 [===========================>..] - ETA: 24s - loss: 1.7849 - acc: 0.3435WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.107905). Check your callbacks.\n",
      "86912/90000 [===========================>..] - ETA: 12s - loss: 1.7767 - acc: 0.3464WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.101094). Check your callbacks.\n",
      "87168/90000 [============================>.] - ETA: 11s - loss: 1.7760 - acc: 0.3466WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100582). Check your callbacks.\n",
      "87296/90000 [============================>.] - ETA: 11s - loss: 1.7757 - acc: 0.3467WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100582). Check your callbacks.\n",
      "87424/90000 [============================>.] - ETA: 10s - loss: 1.7753 - acc: 0.3468WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.101094). Check your callbacks.\n",
      "87552/90000 [============================>.] - ETA: 10s - loss: 1.7749 - acc: 0.3470WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100582). Check your callbacks.\n",
      "87680/90000 [============================>.] - ETA: 9s - loss: 1.7744 - acc: 0.3472 WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100582). Check your callbacks.\n",
      "88448/90000 [============================>.] - ETA: 6s - loss: 1.7725 - acc: 0.3477WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100257). Check your callbacks.\n",
      "88576/90000 [============================>.] - ETA: 5s - loss: 1.7721 - acc: 0.3478WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100654). Check your callbacks.\n",
      "88704/90000 [============================>.] - ETA: 5s - loss: 1.7718 - acc: 0.3480WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100257). Check your callbacks.\n",
      "88832/90000 [============================>.] - ETA: 4s - loss: 1.7715 - acc: 0.3481WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100257). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88960/90000 [============================>.] - ETA: 4s - loss: 1.7712 - acc: 0.3482WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100257). Check your callbacks.\n",
      "90000/90000 [==============================] - 369s 4ms/step - loss: 1.7690 - acc: 0.3491 - val_loss: 1.8091 - val_acc: 0.3667\n",
      "Epoch 2/1000\n",
      "90000/90000 [==============================] - 45s 495us/step - loss: 1.4302 - acc: 0.4785 - val_loss: 1.7106 - val_acc: 0.4060s - loss:  - ETA: 0s - loss: 1.4304 - acc: 0.\n",
      "Epoch 3/1000\n",
      "90000/90000 [==============================] - 39s 435us/step - loss: 1.2945 - acc: 0.5304 - val_loss: 1.2789 - val_acc: 0.5361\n",
      "Epoch 4/1000\n",
      "90000/90000 [==============================] - 39s 432us/step - loss: 1.2164 - acc: 0.5626 - val_loss: 1.2179 - val_acc: 0.5695\n",
      "Epoch 5/1000\n",
      "90000/90000 [==============================] - 38s 426us/step - loss: 1.1639 - acc: 0.5842 - val_loss: 1.2392 - val_acc: 0.54893 - acc:  - ETA: 24s - lo - ETA: 20s - loss: 1.1656 - acc: 0.58 - ETA: 20s - loss:  - E - ETA: 3s - loss: 1.1662 - acc: 0 - ETA: 2s - loss: 1.1659 - acc: 0. - ETA: 2s - loss: 1.1656 - acc:  - ETA: 1s - loss: 1.1656 - acc: 0.5 - ETA: 1s - loss: 1.1657 - acc: 0.5 - ETA: 1s - loss: 1.1651 - a - ETA: 0s - loss: 1.1638 - acc: 0.58\n",
      "Epoch 6/1000\n",
      "90000/90000 [==============================] - 38s 419us/step - loss: 1.1260 - acc: 0.5971 - val_loss: 1.5505 - val_acc: 0.4469ETA: 35s - loss - ETA: 32s - loss - E - - ETA: 12  - ETA: 9s - loss: 1.1255  - ETA: 8s - loss: 1.1258 - acc: 0.59 - ETA: 8s - loss: 1.1263 - ETA: 6s - loss: 1.1239 - ac - ETA: 5 - ETA: 3s - loss - ETA: 0s - loss: 1.1257 - \n",
      "Epoch 7/1000\n",
      "90000/90000 [==============================] - 38s 419us/step - loss: 1.1005 - acc: 0.6071 - val_loss: 1.0896 - val_acc: 0.6096 30s - loss: 1.0797 - acc - ETA: 31s - loss: 1. - ETA: 27s - loss: 1. - ETA: 21s - loss: 1.09 - ETA - - ETA: 15s - loss: 1.0961 - - ETA: 14s - loss: 1.0983 - ETA: 10s - loss: 1.0987 - acc: 0. - ETA: 9s - loss: 1.0993 - acc: 0.607 - ETA: 9s - loss: 1.0997 - acc - ETA: 9s - loss: 1.1014 - acc: 0. - ET - ETA: 5s - loss: 1.1014 - acc: 0.60 - ETA:  - ETA: 2s - loss: 1.1007 - acc: 0. - ETA: 2s - loss: 1.1006 - - ETA: 0s - loss: 1.1008 - a\n",
      "Epoch 8/1000\n",
      "90000/90000 [==============================] - 39s 432us/step - loss: 1.0802 - acc: 0.6151 - val_loss: 1.3802 - val_acc: 0.5679 1.0836 -  - ETA: 4s - loss: 1.0827 - a\n",
      "Epoch 9/1000\n",
      "90000/90000 [==============================] - 39s 437us/step - loss: 1.0686 - acc: 0.6219 - val_loss: 1.2694 - val_acc: 0.5853 1.0641 - acc: 0.6 - ETA: 9s - loss: 1.0640 - acc: 0. - ETA: 8s - loss: 1.0635 - acc: - ETA: 7s - loss: 1.0637 - ac - ETA: 7s - loss: 1.0647 - acc:  - ETA: 6s - loss: 1.0649 - acc: 0. - ETA: 6s - loss: 1.0646 - acc: 0. - ETA: 5s - loss: 1\n",
      "Epoch 10/1000\n",
      "90000/90000 [==============================] - 38s 418us/step - loss: 1.0597 - acc: 0.6279 - val_loss: 1.4375 - val_acc: 0.5539- ETA: 10s - loss:  - ETA: 9s - loss: 1.0579 - acc: 0.6 - ETA: 9s - loss: 1.0578 - acc: 0 - ETA: 9s - loss: 1.0576 - acc: 0.62 - ETA: - ETA: 6s - loss - ETA: 3s - loss: 1.0595 - acc - ETA: 3s - loss: 1.0587  - ETA: 1s - loss: 1.0582 - ac - ETA: 1s - loss: 1.0593 - acc: 0.6 - ETA: 0s - loss: 1.0594 - ac\n",
      "Epoch 11/1000\n",
      "90000/90000 [==============================] - 38s 418us/step - loss: 1.0514 - acc: 0.6286 - val_loss: 1.2037 - val_acc: 0.580571 - ETA: 4s -  - ETA: 2s - loss: 1.0509 - acc: - ETA: 1s - loss: \n",
      "Epoch 12/1000\n",
      "90000/90000 [==============================] - 38s 417us/step - loss: 1.0478 - acc: 0.6321 - val_loss: 1.0381 - val_acc: 0.6319A: 10s - loss: 1.0373 - acc: 0. - ETA: 10s - loss: 1.0371  - ETA: 9s - loss: 1.0 - ETA: 7s - loss: 1.0396 - acc: - ETA: 7s - loss: 1.040 - ETA: 5s - loss:  - ETA: 4s -  - ETA: 1s - loss: 1.\n",
      "Epoch 13/1000\n",
      "90000/90000 [==============================] - 38s 420us/step - loss: 1.0524 - acc: 0.6309 - val_loss: 1.1527 - val_acc: 0.6023\n",
      "Epoch 14/1000\n",
      "90000/90000 [==============================] - 38s 418us/step - loss: 1.0564 - acc: 0.6309 - val_loss: 1.0671 - val_acc: 0.6239 acc: 0. - ETA: 3s - loss: 1.0536 - - ETA: 2s - loss:  - ETA: 0s - loss: 1.0563 - acc: 0.631 - ETA: 0s - loss: 1.0563 - acc: 0\n",
      "Epoch 15/1000\n",
      "90000/90000 [==============================] - 40s 450us/step - loss: 1.0574 - acc: 0.6323 - val_loss: 1.1752 - val_acc: 0.5911 - ETA: 4s - loss: 1.0581 - acc:  - ETA: 3 - ETA: 0s - loss: 1.0577 - acc: 0.632 - ETA: 0s - loss: 1.0578 - acc: \n",
      "Epoch 16/1000\n",
      "90000/90000 [==============================] - 38s 420us/step - loss: 1.0594 - acc: 0.6304 - val_loss: 1.3233 - val_acc: 0.5961\n",
      "Epoch 17/1000\n",
      "90000/90000 [==============================] - 38s 418us/step - loss: 1.0688 - acc: 0.6287 - val_loss: 1.0838 - val_acc: 0.6245 acc: - ETA: 9s - loss: 1.\n",
      "Epoch 18/1000\n",
      "90000/90000 [==============================] - 38s 419us/step - loss: 1.0733 - acc: 0.6283 - val_loss: 1.3131 - val_acc: 0.6058 loss: 1.073\n",
      "Epoch 19/1000\n",
      "90000/90000 [==============================] - 38s 418us/step - loss: 1.0826 - acc: 0.6235 - val_loss: 1.1103 - val_acc: 0.6342s: 1.0817 - acc: 0.\n",
      "Epoch 20/1000\n",
      "90000/90000 [==============================] - 38s 417us/step - loss: 1.0829 - acc: 0.6267 - val_loss: 1.1063 - val_acc: 0.6221s -  - ETA: 4s - loss: 1.0790 - - ETA: 3s -  - ETA: 1s - loss: 1.0822 \n",
      "Epoch 21/1000\n",
      "90000/90000 [==============================] - 38s 420us/step - loss: 1.0971 - acc: 0.6186 - val_loss: 1.1967 - val_acc: 0.6316\n",
      "Epoch 22/1000\n",
      "90000/90000 [==============================] - 38s 422us/step - loss: 1.1022 - acc: 0.6185 - val_loss: 1.2158 - val_acc: 0.5720loss: 1.1023 - acc: 0.618\n",
      "test loss: 1.2248692347420587\n",
      "test accuracy: 0.5694111111111111\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "history = nn.fit(x_train,y_train,batch_size=128,epochs=1000, validation_data=(x_valid, y_valid), verbose=1, callbacks=cbacks)\n",
    "\n",
    "#Evaluate the model with test set\n",
    "score = nn.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.86      0.69      9000\n",
      "           1       0.70      0.76      0.73      9000\n",
      "           2       0.57      0.55      0.56      9000\n",
      "           3       0.47      0.46      0.47      9000\n",
      "           4       0.52      0.51      0.51      9000\n",
      "           5       0.50      0.39      0.44      9000\n",
      "           6       0.92      0.43      0.59      9000\n",
      "           7       0.53      0.82      0.64      9000\n",
      "           8       0.74      0.65      0.69      9000\n",
      "           9       0.77      0.61      0.68      9000\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     90000\n",
      "   macro avg       0.63      0.61      0.60     90000\n",
      "weighted avg       0.63      0.61      0.60     90000\n",
      "\n",
      "[[7734  162  180   52   46   37    5  196  502   86]\n",
      " [ 513 6879   48   38   15   52    0  207  279  969]\n",
      " [1072   68 4905  633  860  473  103  636  229   21]\n",
      " [ 338   89  558 4157  963 1611  124  945  148   67]\n",
      " [ 476   43  557  615 4586  497   42 1966  180   38]\n",
      " [ 333  114  592 1237 1070 3530   45 1880  134   65]\n",
      " [ 252   94 1274 1757  821  423 3909  247  193   30]\n",
      " [ 297   58  174  170  403  297    3 7417   95   86]\n",
      " [1696  394  256  117   80   74    8  279 5850  246]\n",
      " [ 689 1985   47   66   27   65    2  301  327 5491]]\n"
     ]
    }
   ],
   "source": [
    "##Store Plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('6_cnn_accuracy.pdf')\n",
    "plt.close()\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.savefig('6_cnn_loss.pdf')\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "Y_pred = nn.predict(x_test)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print( 'Analysis of results' )\n",
    "target_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "builds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
